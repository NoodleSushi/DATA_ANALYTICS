{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d66f5c-beae-4257-b22c-5961f2f5a432",
   "metadata": {},
   "source": [
    "$\\textbf{PROGRAMMING ASSIGNMENT}$\n",
    "---\n",
    "Instructions:\n",
    "1. Choose a unique dataset from: https://archive.ics.uci.edu\n",
    "2. Your dataset should be unique from your classmates otherwise no points will be given for this exercise.\n",
    "### Complete the Tasks in bold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1f876e-a3bf-4852-b8eb-427a5e0a9065",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "**TASK: Create a correlation heatmap.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab4937-3b04-4c85-96c6-e6eb9ed8c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fetch dataset\n",
    "student_performance = fetch_ucirepo(id=320)\n",
    "\n",
    "# Features data\n",
    "X = student_performance.data.features\n",
    "y = student_performance.data.target\n",
    "\n",
    "# Convert features data to DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "# Check for columns with string values\n",
    "columns_with_strings = df.select_dtypes(include=['object']).columns\n",
    "print(\"Columns with string values:\", columns_with_strings)\n",
    "\n",
    "# Handle categorical variables: Perform one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=['school', 'sex', 'address', 'famsize', 'Pstatus', 'Mjob', 'Fjob',\n",
    "                                        'reason', 'guardian', 'schoolsup', 'famsup', 'paid', 'activities',\n",
    "                                        'nursery', 'higher', 'internet', 'romantic'])\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_encoded.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505afec6-ad20-478f-aa50-3880ddaf5e35",
   "metadata": {},
   "source": [
    "**TASK: What are the top 5 correlated frequencies with the target\\label?**\n",
    "\n",
    "*Note: You many need to map the label to 0s and 1s.*\n",
    "\n",
    "*Additional Note: We're looking for **absolute** correlation values.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3850a3-ac2b-4724-9fcd-d43e5f6f0ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between each feature and the target label\n",
    "correlation_with_target = df_encoded.corrwith(X.iloc[:, -1])\n",
    "\n",
    "# Get the absolute correlation values\n",
    "absolute_correlation = correlation_with_target.abs()\n",
    "\n",
    "# Sort the correlation values in descending order\n",
    "sorted_correlation = absolute_correlation.sort_values(ascending=False)\n",
    "\n",
    "# Get the top 5 correlated features\n",
    "top_correlated_features = sorted_correlation.head(5)\n",
    "\n",
    "print(\"Top 5 correlated features with the target label:\")\n",
    "print(top_correlated_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7a4ed6-a0f0-4ce0-938b-6eb74f1b19d1",
   "metadata": {},
   "source": [
    "## Train | Test Split\n",
    "\n",
    "Our approach here will be one of using Cross Validation on 90% of the dataset, and then judging our results on a final test set of 10% to evaluate our model.\n",
    "\n",
    "**TASK: Split the data into features and labels, and then split into a training set and test set, with 90% for Cross-Validation training, and 10% for a final test set.**\n",
    "\n",
    "*Note: Do not forget to put a random_state for reproducibility.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac9a79-5587-4a7c-912a-391d478cb1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Map the label column to 0s and 1s\n",
    "y_mapped = df_encoded.iloc[:, -1].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Splitting the data into features and labels\n",
    "X = df_encoded  # Features\n",
    "y = y_mapped  # Labels\n",
    "\n",
    "# Splitting the data into training and test sets (90% for training, 10% for testing)\n",
    "X_train_cv, X_test, y_train_cv, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
    "\n",
    "# Printing the shapes of the resulting datasets\n",
    "print(\"Shape of X_train_cv:\", X_train_cv.shape)\n",
    "print(\"Shape of y_train_cv:\", y_train_cv.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a310e-12d1-4006-a29c-5a90b0df00db",
   "metadata": {},
   "source": [
    "**TASK: Create a PipeLine that contains both a StandardScaler and a KNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8125259f-e522-4101-87fa-cba2cedf0b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a pipeline with StandardScaler and KNN model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # StandardScaler for scaling the features\n",
    "    ('knn', KNeighborsClassifier())  # KNN model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff311b09-2750-459a-b7fd-9c585a3e1af0",
   "metadata": {},
   "source": [
    "**TASK: Perform a grid-search with the pipeline to test various values of k and report back the best performing parameters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a9ae5-caa7-4bfe-b711-5ad7a593a1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for KNN\n",
    "param_grid = {\n",
    "    'knn__n_neighbors': [3, 5, 7, 9, 11]  # Various values of k\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Drop rows with missing values from both X_train_cv and y_train_cv\n",
    "X_train_cv_clean = X_train_cv.dropna()\n",
    "y_train_cv_clean = y_train_cv[X_train_cv_clean.index]\n",
    "\n",
    "# Perform the grid search with the cleaned data\n",
    "grid_search.fit(X_train_cv_clean, y_train_cv_clean)\n",
    "\n",
    "# Report back the best performing parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
